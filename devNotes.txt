TODO

   make proper unit test for deep copy

   why does dodging not always work
      Need to take KOs into account when evaluating!  not just score
         can i try to reproduce this and confirm it works 
            (maybe reference bot with specific seed)
            or just reference bot with my health real low

    add verifier test for engine
       compare with previous bots after doing this (and of course fixing any issues that arose)
       upload after doing this if better


   low hanging fruit improvements (update and then compare against previous bot and ref bot)
      Need to avoid getting lost
         try using seed/jippo'd game state to reproduce this
         one solution - bias towards moving in a direction
         another solution - low confidence, go for heuristic 
            aim for big clump of dirt if health looking bad
            go for enemies if health looking good
            run away if at a points advantage
      upload after doing this if better
      
    Tuning
       make bot tunable via config file
       update tournament script
       upload after doing this if better

    add new rules to engine 
       compare with previous bots after doing this
       upload after doing this if better
       
    spreadsheet profile of opponents
       scrape logs to find their output
   
    optimise engine
        next steps:
            return just directions, create move/dig just before advancestate
            possibly bulk allocate list of all move/dig moves at startup

    runmatch script update
       look at using same seed for all matches?

    sort out deployment
        known issue: with cell.cpp (global method)
        known issue: with std::ceil line
           c++ std?
        known issue: incorrect case in header includes
        copy bot folder
        drag in gameengine and random lib
        remove bin/rounds
        make changes to makefile (folders)
        get rid of all cerr?
           use a log wrapper function that can be disabled
         make sure its not a debug build

BRAINSTORMING---------------------------------------------------------------------------------------

    try
       playthroughs considering only nearby worms
        if theres a tie choose random
        monte carlo with evaluation
            bias moves in favour of continuing in the same direction
            getting invalid moves
            check which one is better, trim or no
            returns scores from playthroughs (not just +/-1)
            run sims on a per worm basis
            run matches with extended play time
                confirm that longer playtime makes smarter bot
                check if bot wins sooner with longer play time?
        minimax with evaluation
            plug in strategies for each player
            learn/estimate strategy while you play??
        "mixed" strategy?

        if you don't use select early on you've wasted it
         check if im just using +-1 for playthroughs - should prob add bias there
         disregard moves that would result in instant loss...
         penalise playthroughs that result in loss heavily
         do the move-in-a-line bias for yourself and the other worms

         (the opponent worms, during the playthrough)
         Prune the search tree by recognising symmetries

insights from replays-----------------------------------------------------------------
        bots can get "lost" if they are in wide open spaces
            check confidence of next move, use heuristics if not sure?
            happened against nico beukes
        reference bot strategy is quite popular
        
latest profiling of performance unit test:--------------------------------------------
    GetRandomValidMoveForWorm 
    GetValidMovesForWorm 
        new
        new
        new
        ?
        ?
        Cell_at


    ApplyPowerups
        Cell_at
            IsOnMap

    updatewincondition

    ShootCommand - execute

results analysis --------------------------------------------------------------------
increasing depth to 24 - no noticable improvement
   reducing to 15 - seems to actually be a bit worse
   reducing to 7 - noticably better than jp2, and does better against reference bot

using scorediff, and modifying c to 4, putting playthrough depth back to 24
   lost every game think i did it wrong....
   looking at UCT scores, looks like it converges too quickly, so try raising c to 8
   Need to get back to this after bug fixes

