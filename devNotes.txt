TODO

   add worm name to logs to distinguish
   run grep on match logs
   run compare over LOTS of matches
   make monte carlo choose random if tie

   add per-worm playthroughs
      not just for performance - other worms actually generate noise

   consecutivedonothing count for opponent isnt in file!
   previousposition is wrong when loading file!

   explicit exclude of potential game ending move
      explicit exclude of all chances for opponent to shoot?

   make proper unit test for deep copy

   why does dodging not always work
      i think it's just the monte carlo that is dof
      Need to take KOs into account when evaluating!  not just score
         can i try to reproduce this and confirm it works 
            (maybe reference bot with specific seed)
            or just reference bot with my health real low

   low hanging fruit improvements (update and then compare against previous bot and ref bot)
      Need to avoid getting lost
         try using seed/jippo'd game state to reproduce this
         one solution - bias towards moving in a direction
         another solution - low confidence, go for heuristic 
            aim for big clump of dirt if health looking bad
            go for enemies if health looking good
            run away if at a points advantage
      upload after doing this if better
      
    Tuning
       make bot tunable via config file
       update tournament script
       upload after doing this if better

    add new rules to engine 
       compare with previous bots after doing this
       upload after doing this if better
       
    spreadsheet profile of opponents
       scrape logs to find their output
   
    optimise engine
        next steps:
            return just directions, create move/dig just before advancestate
            possibly bulk allocate list of all move/dig moves at startup

    runmatch script update
       look at using same seed for all matches?

    sort out deployment
        known issue: with cell.cpp (global method)
        known issue: with std::ceil line
           c++ std?
        known issue: incorrect case in header includes
        copy bot folder
        drag in gameengine and random lib
        remove bin/rounds
        make changes to makefile (folders)
        get rid of all cerr?
           use a log wrapper function that can be disabled
         make sure its not a debug build
         undefine EXCEPTION_ON_ERROR

MISC NOTES--------------------
to scan match logs for No Commands:

find . -wholename "*JP3/PlayerCommand*" -exec grep -A 1 "No Command" {} +
find . -wholename "*JP3/PlayerCommand*" -exec grep -A 1 "nothing" {} +

BRAINSTORMING---------------------------------------------------------------------------------------

    try
       playthroughs considering only nearby worms
        if theres a tie choose random
        monte carlo with evaluation
            bias moves in favour of continuing in the same direction
            getting invalid moves
            check which one is better, trim or no
            returns scores from playthroughs (not just +/-1)
            run sims on a per worm basis
            run matches with extended play time
                confirm that longer playtime makes smarter bot
                check if bot wins sooner with longer play time?
        minimax with evaluation
            plug in strategies for each player
            learn/estimate strategy while you play??
        "mixed" strategy?

        coward worms? make the enemy waste time

        if you don't use select early on you've wasted it
         check if im just using +-1 for playthroughs - should prob add bias there
         disregard moves that would result in instant loss...
         penalise playthroughs that result in loss heavily
         do the move-in-a-line bias for yourself and the other worms

         (the opponent worms, during the playthrough)
         Prune the search tree by recognising symmetries

insights from replays-----------------------------------------------------------------
        bots can get "lost" if they are in wide open spaces
            check confidence of next move, use heuristics if not sure?
            happened against nico beukes
        reference bot strategy is quite popular
        
latest profiling of performance unit test:--------------------------------------------


bot description --------------------------------------------------------------------
jp2 - what i submitted for Early Bird.  basic monte carlo, buggy
jp3 - jp2 with various bug fixes
jp4 - jp3 with optimizations to GetRandomMoveForWorm and added multithreading.  Plies/s up from ~600k to 1.2M

results analysis --------------------------------------------------------------------
increasing depth to 24 - no noticable improvement
   reducing to 15 - seems to actually be a bit worse
   reducing to 7 - noticably better than jp2, and does better against reference bot

using scorediff, and modifying c to 4, putting playthrough depth back to 24
   lost every game think i did it wrong....
   looking at UCT scores, looks like it converges too quickly, so try raising c to 8
   Need to get back to this after bug fixes

upped the time limit to 3s, improvement not great.  beats reference bot and 1s jp2 66% of the time
   (wins/draws/losses):     reference-bot            jp2                      jp3                      
   reference-bot:           0/0/0                    15/0/5                   5/0/15                   (20/0/20)
   jp2:                     5/0/15                   0/0/0                    6/0/14                   (11/0/29)
   jp3:                     15/0/5                   14/0/6                   0/0/0                    (29/0/11)

after fixing some edge case bugs in the engine and putting playthrough depth back to 24 (engine version 2019.2.1):
   (wins/draws/losses):     reference-bot            jp2                      jp3                      
   reference-bot:           0/0/0                    84/0/53                  70/0/67                  (154/0/120)
   jp2:                     53/0/84                  0/0/0                    17/0/120                 (70/0/204)
   jp3:                     67/0/70                  120/0/17                 0/0/0                    (187/0/87)

after optimising engine and adding multithreading (depth still 24) (engine version 2019.2.2)
(wins/draws/losses):     reference-bot            jp3                      jp2                      
reference-bot:           0/0/0                    10/0/23                  17/0/16                  (27/0/39)
jp3:                     23/0/10                  0/0/0                    19/0/14                  (42/0/24)
jp2:                     16/0/17                  14/0/19                  0/0/0                    (30/0/36)

some more of the same^ but redirecting std::err to /dev/null (suspect foul play by jp2)

(wins/draws/losses):     reference-bot            jp3                      jp2                      
reference-bot:           0/0/0                    16/0/16                  16/0/16                  (32/0/32)
jp3:                     16/0/16                  0/0/0                    20/0/12                  (36/0/28)
jp2:                     16/0/16                  12/0/20                  0/0/0                    (28/0/36)

bringing back the old one as jp3 and making current one jp4:
